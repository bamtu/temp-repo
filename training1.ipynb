{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed04dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 16:55:28.492065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-19 16:55:28.492098: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/competition/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from lightgbm import LGBMClassifier\n",
    "import re\n",
    "import joblib\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "\n",
    "def rmse(y, pred):\n",
    "    return np.sqrt(mean_squared_error(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f6c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/training_data/preprocessed_180_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3836e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b6239c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>proton_density</th>\n",
       "      <th>proton_temperature</th>\n",
       "      <th>proton_speed</th>\n",
       "      <th>bx_gsm</th>\n",
       "      <th>by_gsm</th>\n",
       "      <th>bz_gsm</th>\n",
       "      <th>bt</th>\n",
       "      <th>...</th>\n",
       "      <th>In_D</th>\n",
       "      <th>In_I</th>\n",
       "      <th>Jj_X</th>\n",
       "      <th>Jj_Y</th>\n",
       "      <th>Jj_Z</th>\n",
       "      <th>Jj_H</th>\n",
       "      <th>Jj_F</th>\n",
       "      <th>Jj_D</th>\n",
       "      <th>Jj_I</th>\n",
       "      <th>Dindex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.50</td>\n",
       "      <td>5.97</td>\n",
       "      <td>63349.11</td>\n",
       "      <td>374.65</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>4.71</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>5.35</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>53.53</td>\n",
       "      <td>33215.99</td>\n",
       "      <td>-286.17</td>\n",
       "      <td>31620.31</td>\n",
       "      <td>33217.22</td>\n",
       "      <td>45860.96</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>43.59</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>269.50</td>\n",
       "      <td>5.99</td>\n",
       "      <td>59161.81</td>\n",
       "      <td>370.63</td>\n",
       "      <td>-2.47</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>6.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.23</td>\n",
       "      <td>53.53</td>\n",
       "      <td>33209.00</td>\n",
       "      <td>-313.60</td>\n",
       "      <td>31614.71</td>\n",
       "      <td>33210.48</td>\n",
       "      <td>45852.22</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>43.59</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>449.50</td>\n",
       "      <td>5.62</td>\n",
       "      <td>69938.55</td>\n",
       "      <td>377.34</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>7.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.23</td>\n",
       "      <td>53.53</td>\n",
       "      <td>33210.12</td>\n",
       "      <td>-309.23</td>\n",
       "      <td>31621.87</td>\n",
       "      <td>33211.56</td>\n",
       "      <td>45857.95</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>43.60</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>629.50</td>\n",
       "      <td>5.61</td>\n",
       "      <td>83242.99</td>\n",
       "      <td>403.80</td>\n",
       "      <td>2.07</td>\n",
       "      <td>7.53</td>\n",
       "      <td>-6.16</td>\n",
       "      <td>11.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.21</td>\n",
       "      <td>53.54</td>\n",
       "      <td>33211.10</td>\n",
       "      <td>-303.70</td>\n",
       "      <td>31622.53</td>\n",
       "      <td>33212.49</td>\n",
       "      <td>45859.06</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>43.60</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>809.50</td>\n",
       "      <td>5.70</td>\n",
       "      <td>170966.78</td>\n",
       "      <td>445.52</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>8.20</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>13.91</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.21</td>\n",
       "      <td>53.58</td>\n",
       "      <td>33161.93</td>\n",
       "      <td>-302.83</td>\n",
       "      <td>31623.31</td>\n",
       "      <td>33163.32</td>\n",
       "      <td>45824.00</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>43.64</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23371</th>\n",
       "      <td>23371</td>\n",
       "      <td>23371</td>\n",
       "      <td>4206869.50</td>\n",
       "      <td>6.58</td>\n",
       "      <td>89799.83</td>\n",
       "      <td>438.73</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.02</td>\n",
       "      <td>4.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.79</td>\n",
       "      <td>33215.36</td>\n",
       "      <td>-652.36</td>\n",
       "      <td>31963.44</td>\n",
       "      <td>33221.76</td>\n",
       "      <td>46101.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.89</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23372</th>\n",
       "      <td>23372</td>\n",
       "      <td>23372</td>\n",
       "      <td>4207049.50</td>\n",
       "      <td>6.54</td>\n",
       "      <td>87216.37</td>\n",
       "      <td>427.42</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>3.98</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.79</td>\n",
       "      <td>33216.24</td>\n",
       "      <td>-652.49</td>\n",
       "      <td>31964.94</td>\n",
       "      <td>33222.64</td>\n",
       "      <td>46103.16</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.89</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23373</th>\n",
       "      <td>23373</td>\n",
       "      <td>23373</td>\n",
       "      <td>4207229.50</td>\n",
       "      <td>8.22</td>\n",
       "      <td>76142.33</td>\n",
       "      <td>429.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4.34</td>\n",
       "      <td>1.72</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.78</td>\n",
       "      <td>33217.10</td>\n",
       "      <td>-652.82</td>\n",
       "      <td>31966.49</td>\n",
       "      <td>33223.51</td>\n",
       "      <td>46104.87</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.90</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23374</th>\n",
       "      <td>23374</td>\n",
       "      <td>23374</td>\n",
       "      <td>4207409.50</td>\n",
       "      <td>9.48</td>\n",
       "      <td>93696.57</td>\n",
       "      <td>429.54</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>53.78</td>\n",
       "      <td>33223.42</td>\n",
       "      <td>-657.47</td>\n",
       "      <td>31967.08</td>\n",
       "      <td>33229.93</td>\n",
       "      <td>46109.90</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.89</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23375</th>\n",
       "      <td>23375</td>\n",
       "      <td>23375</td>\n",
       "      <td>4207589.50</td>\n",
       "      <td>5.80</td>\n",
       "      <td>80350.05</td>\n",
       "      <td>404.25</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>2.12</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.77</td>\n",
       "      <td>33231.23</td>\n",
       "      <td>-656.54</td>\n",
       "      <td>31965.70</td>\n",
       "      <td>33237.72</td>\n",
       "      <td>46114.55</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.88</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23376 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  proton_density  \\\n",
       "0                 0             0       89.50            5.97   \n",
       "1                 1             1      269.50            5.99   \n",
       "2                 2             2      449.50            5.62   \n",
       "3                 3             3      629.50            5.61   \n",
       "4                 4             4      809.50            5.70   \n",
       "...             ...           ...         ...             ...   \n",
       "23371         23371         23371  4206869.50            6.58   \n",
       "23372         23372         23372  4207049.50            6.54   \n",
       "23373         23373         23373  4207229.50            8.22   \n",
       "23374         23374         23374  4207409.50            9.48   \n",
       "23375         23375         23375  4207589.50            5.80   \n",
       "\n",
       "       proton_temperature  proton_speed  bx_gsm  by_gsm  bz_gsm    bt  ...  \\\n",
       "0                63349.11        374.65   -1.19    4.71   -1.53  5.35  ...   \n",
       "1                59161.81        370.63   -2.47    4.95   -2.63  6.56  ...   \n",
       "2                69938.55        377.34   -3.34    5.26    1.57  7.64  ...   \n",
       "3                83242.99        403.80    2.07    7.53   -6.16 11.85  ...   \n",
       "4               170966.78        445.52   -1.93    8.20   -2.96 13.91  ...   \n",
       "...                   ...           ...     ...     ...     ...   ...  ...   \n",
       "23371            89799.83        438.73    1.41    2.39    2.02  4.05  ...   \n",
       "23372            87216.37        427.42   -0.70    2.13   -1.97  3.98  ...   \n",
       "23373            76142.33        429.93    0.95    4.34    1.72  5.40  ...   \n",
       "23374            93696.57        429.54    1.52    2.38    3.50  5.16  ...   \n",
       "23375            80350.05        404.25   -2.07    2.12    4.83  5.99  ...   \n",
       "\n",
       "       In_D  In_I     Jj_X    Jj_Y     Jj_Z     Jj_H     Jj_F  Jj_D  Jj_I  \\\n",
       "0     -8.18 53.53 33215.99 -286.17 31620.31 33217.22 45860.96 -0.49 43.59   \n",
       "1     -8.23 53.53 33209.00 -313.60 31614.71 33210.48 45852.22 -0.54 43.59   \n",
       "2     -8.23 53.53 33210.12 -309.23 31621.87 33211.56 45857.95 -0.53 43.60   \n",
       "3     -8.21 53.54 33211.10 -303.70 31622.53 33212.49 45859.06 -0.52 43.60   \n",
       "4     -8.21 53.58 33161.93 -302.83 31623.31 33163.32 45824.00 -0.52 43.64   \n",
       "...     ...   ...      ...     ...      ...      ...      ...   ...   ...   \n",
       "23371 -7.94 53.79 33215.36 -652.36 31963.44 33221.76 46101.49 -1.13 43.89   \n",
       "23372 -7.94 53.79 33216.24 -652.49 31964.94 33222.64 46103.16 -1.13 43.89   \n",
       "23373 -7.94 53.78 33217.10 -652.82 31966.49 33223.51 46104.87 -1.13 43.90   \n",
       "23374 -7.95 53.78 33223.42 -657.47 31967.08 33229.93 46109.90 -1.13 43.89   \n",
       "23375 -7.94 53.77 33231.23 -656.54 31965.70 33237.72 46114.55 -1.13 43.88   \n",
       "\n",
       "       Dindex  \n",
       "0        3.00  \n",
       "1        3.00  \n",
       "2        1.00  \n",
       "3        3.00  \n",
       "4        4.00  \n",
       "...       ...  \n",
       "23371    1.00  \n",
       "23372    2.00  \n",
       "23373    3.00  \n",
       "23374    1.00  \n",
       "23375    2.00  \n",
       "\n",
       "[23376 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e1f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['datetime'] = df.datetime.apply(pd.to_datetime)\n",
    "\n",
    "def evaluate_regr(y, pred):\n",
    "    rmse_val = rmse(y, pred)\n",
    "    print('RMSE: ', rmse_val)\n",
    "\n",
    "y_target = df['Dindex']\n",
    "X_features = df.drop(['Dindex', 'Unnamed: 0','Unnamed: 0.1', 'Unnamed: 0.2'], axis=1, inplace=False)\n",
    "input_var = ['proton_density', 'proton_speed', 'proton_temperature', 'bx_gsm', 'by_gsm', 'bz_gsm', 'bt', \n",
    "            'gn_X', 'gn_Y', 'gn_Z', 'gn_H', 'gn_F', 'gn_D', 'gn_I', 'In_X', 'Jj_X', 'Jj_Y', 'Jj_Z', 'Jj_F', 'Jj_I']\n",
    "input_X = X_features[input_var]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_X, y_target, test_size=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92e2182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/quiz/preprocessed.csv')\n",
    "submission = submission.drop(['Unnamed: 0'], axis=1, inplace=False)\n",
    "submission = submission[input_var]\n",
    "pred = xgbc.predict(submission)\n",
    "submission['Dindex'] = pred\n",
    "submission[240:]['Dindex'].to_csv(\"data/submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6dee60ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  1.0621700090875887\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "def evaluate_regr(y, pred):\n",
    "    rmse_val = rmse(y, pred)\n",
    "    print('RMSE: ', rmse_val)\n",
    "\n",
    "#y_data = df.pop('Dindex')\n",
    "#x_data = df\n",
    "y_target = df['Dindex']\n",
    "X_features = df.drop(['Dindex', 'Unnamed0', 'Unnamed02', 'Unnamed01'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.01, random_state=0)\n",
    "\n",
    "lr_reg = LGBMClassifier(n_estimators=500)\n",
    "lr_reg.fit(X_train, y_train)\n",
    "pred = lr_reg.predict(X_test)\n",
    "\n",
    "evaluate_regr(y_test, pred)\n",
    "# joblib.dump(lr_reg, 'lgb.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed622ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm_pickle = joblib.load('lgb.pkl')\n",
    "submission = pd.read_csv('data/quiz/preprocessed.csv')\n",
    "submission = submission.drop(['Unnamed: 0'], axis=1, inplace=False)\n",
    "pred = lr_reg.predict(submission)\n",
    "submission['Dindex'] = pred\n",
    "submission.to_csv(\"data/submit.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0fe4140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     proton_density  proton_temperature  proton_speed    bx_gsm    by_gsm  \\\n",
      "0          9.740889       425579.922222    512.838889  2.062833 -2.555000   \n",
      "1          9.482889       442720.550000    508.746111 -2.598778  0.613778   \n",
      "2          8.170778       379750.555556    500.266111 -2.452833  2.536000   \n",
      "3          9.392056       401559.461111    508.778333 -1.083333  3.821389   \n",
      "4          6.388306       264431.102778    483.449167 -2.386889  3.520472   \n",
      "..              ...                 ...           ...       ...       ...   \n",
      "715        5.114333       234353.788889    466.514444  5.074000 -2.140111   \n",
      "716        7.889639       283785.755556    505.847500  3.460778 -1.112556   \n",
      "717        7.943278       248275.350000    506.611667  2.695278 -2.575889   \n",
      "718        6.976667       246099.133333    503.444444  3.394111 -3.976056   \n",
      "719        6.443056       218375.266667    494.004444  3.749167 -4.214556   \n",
      "\n",
      "       bz_gsm        bt          gn_X        gn_Y          gn_Z  ...  \\\n",
      "0   -3.662000  5.126389  29617.007676 -461.221564  41411.985833  ...   \n",
      "1   -3.691389  4.979056  29597.857856 -478.313018  41407.469667  ...   \n",
      "2   -3.199389  5.010556  29592.070955 -480.436603  41417.539611  ...   \n",
      "3   -3.066611  5.249111  29589.694304 -467.612368  41418.914056  ...   \n",
      "4   -2.895417  5.491139  29594.268855 -462.701717  41419.240500  ...   \n",
      "..        ...       ...           ...         ...           ...  ...   \n",
      "715  0.921611  5.987444  29608.728270 -461.076827  41429.358889  ...   \n",
      "716 -0.879611  5.841389  29619.788914 -461.297837  41429.182500  ...   \n",
      "717  1.120056  5.964556  29615.573879 -449.244186  41428.158833  ...   \n",
      "718  0.353944  5.994056  29612.834348 -451.926146  41430.147111  ...   \n",
      "719 -1.898056  6.205944  29606.216625 -434.650542  41429.903667  ...   \n",
      "\n",
      "             In_F      In_D       In_I          Jj_X        Jj_Y  \\\n",
      "0    50866.508227 -7.946258  53.796871  33223.902985 -656.091390   \n",
      "1    50849.593641 -7.979597  53.809841  33203.067914 -669.672118   \n",
      "2    50853.413647 -7.986871  53.822136  33192.850866 -673.930250   \n",
      "3    50853.596857 -7.960919  53.825628  33192.259426 -661.995723   \n",
      "4    50856.871631 -7.950945  53.821953  33197.098970 -658.201851   \n",
      "..            ...       ...        ...           ...         ...   \n",
      "715  50870.923714 -7.970942  53.813964  33156.095494 -684.022521   \n",
      "716  50877.238756 -7.970528  53.803741  33167.653194 -684.317198   \n",
      "717  50874.596238 -7.947901  53.807967  33164.639499 -673.345880   \n",
      "718  50873.746698 -7.952349  53.810944  33161.533239 -675.470218   \n",
      "719  50870.915089 -7.919943  53.817273  33158.928072 -657.086253   \n",
      "\n",
      "             Jj_Z          Jj_H          Jj_F      Jj_D       Jj_I  \n",
      "0    31952.332833  33230.380945  46099.997767 -1.131305  43.876736  \n",
      "1    31942.454278  33209.821360  46078.332416 -1.155443  43.885602  \n",
      "2    31952.962167  33199.692111  46078.317664 -1.163143  43.903749  \n",
      "3    31956.480833  33198.860556  46080.158548 -1.142571  43.907619  \n",
      "4    31957.081333  33203.623612  46084.006673 -1.135860  43.904050  \n",
      "..            ...           ...           ...       ...        ...  \n",
      "715  31977.506778  33163.151222  46069.030335 -1.181867  43.957258  \n",
      "716  31977.433556  33174.711889  46077.302097 -1.181963  43.947214  \n",
      "717  31979.435278  33171.474611  46076.360875 -1.163123  43.951800  \n",
      "718  31981.381833  33168.411944  46075.506888 -1.166902  43.956186  \n",
      "719  31981.219000  33165.438333  46073.253311 -1.135239  43.958607  \n",
      "\n",
      "[720 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ee0cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       proton_density  proton_temperature  proton_speed    bx_gsm    by_gsm  \\\n",
      "0            5.969653        63349.109764    374.652888 -1.191787  4.710839   \n",
      "1            5.993983        59161.813942    370.626311 -2.469191  4.945273   \n",
      "2            5.618350        69938.545719    377.336518 -3.337570  5.263640   \n",
      "3            5.614182        83242.987856    403.796592  2.066158  7.528629   \n",
      "4            5.704158       170966.782678    445.517097 -1.931976  8.198496   \n",
      "...               ...                 ...           ...       ...       ...   \n",
      "23371        6.578556        89799.827778    438.730556  1.414111  2.388500   \n",
      "23372        6.542222        87216.372222    427.420000 -0.699833  2.133722   \n",
      "23373        8.224667        76142.327778    429.934444  0.952167  4.338000   \n",
      "23374        9.480444        93696.566667    429.536111  1.519444  2.384778   \n",
      "23375        5.804972        80350.052778    404.253611 -2.070778  2.120722   \n",
      "\n",
      "         bz_gsm         bt          gn_X        gn_Y          gn_Z  ...  \\\n",
      "0     -1.527034   5.345930  29632.762656 -111.915032  41079.199167  ...   \n",
      "1     -2.631046   6.555102  29628.979108 -137.780447  41075.497667  ...   \n",
      "2      1.573790   7.643978  29632.270318 -134.260896  41078.963333  ...   \n",
      "3     -6.158649  11.850512  29632.697850 -126.086080  41080.143833  ...   \n",
      "4     -2.955329  13.905808  29585.529232 -124.440944  41082.728278  ...   \n",
      "...         ...        ...           ...         ...           ...  ...   \n",
      "23371  2.020500   4.051278  29628.331264 -458.371316  41412.758944  ...   \n",
      "23372 -1.965222   3.977278  29628.821463 -458.076859  41412.401500  ...   \n",
      "23373  1.718222   5.399778  29629.861981 -458.849086  41413.106556  ...   \n",
      "23374  3.498222   5.160667  29636.647538 -463.617609  41413.725556  ...   \n",
      "23375  4.833278   5.993389  29642.646652 -461.487388  41412.401889  ...   \n",
      "\n",
      "               In_F      In_D       In_I          Jj_X        Jj_Y  \\\n",
      "0      50595.447067 -8.182743  53.534471  33215.986640 -286.173261   \n",
      "1      50587.188304 -8.233980  53.534514  33208.998075 -313.595048   \n",
      "2      50592.178758 -8.227317  53.534587  33210.117560 -309.234511   \n",
      "3      50593.847388 -8.212298  53.535428  33211.098755 -303.696024   \n",
      "4      50568.520367 -8.210063  53.581267  33161.933614 -302.825695   \n",
      "...             ...       ...        ...           ...         ...   \n",
      "23371  50872.340325 -7.940930  53.786197  33215.357193 -652.358914   \n",
      "23372  50872.957098 -7.940346  53.785937  33216.235093 -652.494669   \n",
      "23373  50873.853495 -7.940706  53.784872  33217.098527 -652.823096   \n",
      "23374  50878.490345 -7.948626  53.778802  33223.421903 -657.474714   \n",
      "23375  50881.998628 -7.944350  53.772828  33231.231650 -656.538229   \n",
      "\n",
      "               Jj_Z          Jj_H          Jj_F      Jj_D       Jj_I  \n",
      "0      31620.311056  33217.219667  45860.961199 -0.493620  43.589126  \n",
      "1      31614.710833  33210.479611  45852.218059 -0.541031  43.589864  \n",
      "2      31621.874000  33211.557416  45857.945920 -0.533492  43.595428  \n",
      "3      31622.526944  33212.487333  45859.061856 -0.523922  43.595208  \n",
      "4      31623.305611  33163.316722  45824.001846 -0.523193  43.638309  \n",
      "...             ...           ...           ...       ...        ...  \n",
      "23371  31963.438556  33221.762833  46101.485126 -1.125160  43.894109  \n",
      "23372  31964.940833  33222.643333  46103.161219 -1.125365  43.894696  \n",
      "23373  31966.494944  33223.513056  46104.865493 -1.125902  43.895338  \n",
      "23374  31967.084000  33229.926833  46109.895879 -1.133706  43.890340  \n",
      "23375  31965.697000  33237.716872  46114.553278 -1.131827  43.882394  \n",
      "\n",
      "[23376 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb5acc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 26/29 [01:57<00:21,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:45:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:05<00:00,  4.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>None</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>None</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>None</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>None</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>None</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>None</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.34</td>\n",
       "      <td>None</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>None</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.32</td>\n",
       "      <td>None</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>None</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>None</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.27</td>\n",
       "      <td>None</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.27</td>\n",
       "      <td>None</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.25</td>\n",
       "      <td>None</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>None</td>\n",
       "      <td>0.41</td>\n",
       "      <td>13.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>None</td>\n",
       "      <td>0.42</td>\n",
       "      <td>24.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>None</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.40</td>\n",
       "      <td>38.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.19</td>\n",
       "      <td>None</td>\n",
       "      <td>0.39</td>\n",
       "      <td>22.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>None</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.18</td>\n",
       "      <td>None</td>\n",
       "      <td>0.37</td>\n",
       "      <td>6.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.18</td>\n",
       "      <td>None</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.14</td>\n",
       "      <td>None</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                        \n",
       "NearestCentroid                  0.23               0.43    None      0.23   \n",
       "LinearDiscriminantAnalysis       0.49               0.40    None      0.43   \n",
       "GaussianNB                       0.41               0.39    None      0.40   \n",
       "AdaBoostClassifier               0.27               0.36    None      0.27   \n",
       "ExtraTreesClassifier             0.51               0.35    None      0.49   \n",
       "XGBClassifier                    0.50               0.34    None      0.48   \n",
       "RandomForestClassifier           0.51               0.34    None      0.49   \n",
       "BaggingClassifier                0.46               0.34    None      0.45   \n",
       "DecisionTreeClassifier           0.40               0.32    None      0.40   \n",
       "LGBMClassifier                   0.43               0.32    None      0.42   \n",
       "SGDClassifier                    0.42               0.28    None      0.40   \n",
       "ExtraTreeClassifier              0.41               0.27    None      0.41   \n",
       "Perceptron                       0.34               0.27    None      0.32   \n",
       "KNeighborsClassifier             0.43               0.25    None      0.42   \n",
       "LabelPropagation                 0.41               0.25    None      0.41   \n",
       "LabelSpreading                   0.42               0.25    None      0.42   \n",
       "LogisticRegression               0.49               0.23    None      0.44   \n",
       "SVC                              0.49               0.20    None      0.40   \n",
       "PassiveAggressiveClassifier      0.36               0.20    None      0.36   \n",
       "BernoulliNB                      0.38               0.20    None      0.37   \n",
       "CalibratedClassifierCV           0.48               0.19    None      0.39   \n",
       "RidgeClassifierCV                0.48               0.18    None      0.38   \n",
       "LinearSVC                        0.47               0.18    None      0.37   \n",
       "RidgeClassifier                  0.47               0.18    None      0.37   \n",
       "DummyClassifier                  0.42               0.14    None      0.25   \n",
       "\n",
       "                             Time Taken  \n",
       "Model                                    \n",
       "NearestCentroid                    0.02  \n",
       "LinearDiscriminantAnalysis         0.04  \n",
       "GaussianNB                         0.02  \n",
       "AdaBoostClassifier                 1.54  \n",
       "ExtraTreesClassifier               1.87  \n",
       "XGBClassifier                      7.76  \n",
       "RandomForestClassifier             5.52  \n",
       "BaggingClassifier                  1.82  \n",
       "DecisionTreeClassifier             0.32  \n",
       "LGBMClassifier                     0.81  \n",
       "SGDClassifier                      0.36  \n",
       "ExtraTreeClassifier                0.03  \n",
       "Perceptron                         0.08  \n",
       "KNeighborsClassifier               0.18  \n",
       "LabelPropagation                  13.48  \n",
       "LabelSpreading                    24.06  \n",
       "LogisticRegression                 0.84  \n",
       "SVC                               38.36  \n",
       "PassiveAggressiveClassifier        0.08  \n",
       "BernoulliNB                        0.02  \n",
       "CalibratedClassifierCV            22.26  \n",
       "RidgeClassifierCV                  0.03  \n",
       "LinearSVC                          6.01  \n",
       "RidgeClassifier                    0.03  \n",
       "DummyClassifier                    0.01  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose=0, predictions=True)\n",
    "\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e975b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:52:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7396002616336389\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#0.7250 ,1500, 0.1\n",
    "xgbc = XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=700,\n",
    "    max_depth=5,\n",
    "    min_child_weight=5,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    nthread=-1,\n",
    "    seed=2019\n",
    "    )\n",
    "xgbc.fit(X_train, y_train)\n",
    "pred = xgbc.predict(X_test)\n",
    "\n",
    "evaluate_regr(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab29684",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 11, got 28",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/quiz/preprocessed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m submission \u001b[38;5;241m=\u001b[39m submission\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mxgbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDindex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m      5\u001b[0m submission[\u001b[38;5;241m240\u001b[39m:][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDindex\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/submit.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1284\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1277\u001b[0m     X: array_like,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     iteration_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1283\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 1284\u001b[0m     class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mntree_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mntree_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:881\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m    890\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m     \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/core.py:2025\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2021\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2022\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2023\u001b[0m         )\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m         )\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _array_interface\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 11, got 28"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('data/quiz/preprocessed.csv')\n",
    "submission = submission.drop(['Unnamed: 0'], axis=1, inplace=False)\n",
    "pred = xgbc.predict(submission)\n",
    "submission['Dindex'] = pred\n",
    "submission[240:]['Dindex'].to_csv(\"data/submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f657e01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proton_density</th>\n",
       "      <th>proton_temperature</th>\n",
       "      <th>proton_speed</th>\n",
       "      <th>bx_gsm</th>\n",
       "      <th>by_gsm</th>\n",
       "      <th>bz_gsm</th>\n",
       "      <th>bt</th>\n",
       "      <th>gn_X</th>\n",
       "      <th>gn_Y</th>\n",
       "      <th>gn_Z</th>\n",
       "      <th>...</th>\n",
       "      <th>In_F</th>\n",
       "      <th>In_D</th>\n",
       "      <th>In_I</th>\n",
       "      <th>Jj_X</th>\n",
       "      <th>Jj_Y</th>\n",
       "      <th>Jj_Z</th>\n",
       "      <th>Jj_H</th>\n",
       "      <th>Jj_F</th>\n",
       "      <th>Jj_D</th>\n",
       "      <th>Jj_I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.97</td>\n",
       "      <td>63349.11</td>\n",
       "      <td>374.65</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>4.71</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>5.35</td>\n",
       "      <td>29632.76</td>\n",
       "      <td>-111.92</td>\n",
       "      <td>41079.20</td>\n",
       "      <td>...</td>\n",
       "      <td>50595.45</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>53.53</td>\n",
       "      <td>33215.99</td>\n",
       "      <td>-286.17</td>\n",
       "      <td>31620.31</td>\n",
       "      <td>33217.22</td>\n",
       "      <td>45860.96</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>43.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.99</td>\n",
       "      <td>59161.81</td>\n",
       "      <td>370.63</td>\n",
       "      <td>-2.47</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>6.56</td>\n",
       "      <td>29628.98</td>\n",
       "      <td>-137.78</td>\n",
       "      <td>41075.50</td>\n",
       "      <td>...</td>\n",
       "      <td>50587.19</td>\n",
       "      <td>-8.23</td>\n",
       "      <td>53.53</td>\n",
       "      <td>33209.00</td>\n",
       "      <td>-313.60</td>\n",
       "      <td>31614.71</td>\n",
       "      <td>33210.48</td>\n",
       "      <td>45852.22</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>43.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.62</td>\n",
       "      <td>69938.55</td>\n",
       "      <td>377.34</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>7.64</td>\n",
       "      <td>29632.27</td>\n",
       "      <td>-134.26</td>\n",
       "      <td>41078.96</td>\n",
       "      <td>...</td>\n",
       "      <td>50592.18</td>\n",
       "      <td>-8.23</td>\n",
       "      <td>53.53</td>\n",
       "      <td>33210.12</td>\n",
       "      <td>-309.23</td>\n",
       "      <td>31621.87</td>\n",
       "      <td>33211.56</td>\n",
       "      <td>45857.95</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>43.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.61</td>\n",
       "      <td>83242.99</td>\n",
       "      <td>403.80</td>\n",
       "      <td>2.07</td>\n",
       "      <td>7.53</td>\n",
       "      <td>-6.16</td>\n",
       "      <td>11.85</td>\n",
       "      <td>29632.70</td>\n",
       "      <td>-126.09</td>\n",
       "      <td>41080.14</td>\n",
       "      <td>...</td>\n",
       "      <td>50593.85</td>\n",
       "      <td>-8.21</td>\n",
       "      <td>53.54</td>\n",
       "      <td>33211.10</td>\n",
       "      <td>-303.70</td>\n",
       "      <td>31622.53</td>\n",
       "      <td>33212.49</td>\n",
       "      <td>45859.06</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>43.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.70</td>\n",
       "      <td>170966.78</td>\n",
       "      <td>445.52</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>8.20</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>13.91</td>\n",
       "      <td>29585.53</td>\n",
       "      <td>-124.44</td>\n",
       "      <td>41082.73</td>\n",
       "      <td>...</td>\n",
       "      <td>50568.52</td>\n",
       "      <td>-8.21</td>\n",
       "      <td>53.58</td>\n",
       "      <td>33161.93</td>\n",
       "      <td>-302.83</td>\n",
       "      <td>31623.31</td>\n",
       "      <td>33163.32</td>\n",
       "      <td>45824.00</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>43.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23371</th>\n",
       "      <td>6.58</td>\n",
       "      <td>89799.83</td>\n",
       "      <td>438.73</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.02</td>\n",
       "      <td>4.05</td>\n",
       "      <td>29628.33</td>\n",
       "      <td>-458.37</td>\n",
       "      <td>41412.76</td>\n",
       "      <td>...</td>\n",
       "      <td>50872.34</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.79</td>\n",
       "      <td>33215.36</td>\n",
       "      <td>-652.36</td>\n",
       "      <td>31963.44</td>\n",
       "      <td>33221.76</td>\n",
       "      <td>46101.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23372</th>\n",
       "      <td>6.54</td>\n",
       "      <td>87216.37</td>\n",
       "      <td>427.42</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>3.98</td>\n",
       "      <td>29628.82</td>\n",
       "      <td>-458.08</td>\n",
       "      <td>41412.40</td>\n",
       "      <td>...</td>\n",
       "      <td>50872.96</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.79</td>\n",
       "      <td>33216.24</td>\n",
       "      <td>-652.49</td>\n",
       "      <td>31964.94</td>\n",
       "      <td>33222.64</td>\n",
       "      <td>46103.16</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23373</th>\n",
       "      <td>8.22</td>\n",
       "      <td>76142.33</td>\n",
       "      <td>429.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4.34</td>\n",
       "      <td>1.72</td>\n",
       "      <td>5.40</td>\n",
       "      <td>29629.86</td>\n",
       "      <td>-458.85</td>\n",
       "      <td>41413.11</td>\n",
       "      <td>...</td>\n",
       "      <td>50873.85</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.78</td>\n",
       "      <td>33217.10</td>\n",
       "      <td>-652.82</td>\n",
       "      <td>31966.49</td>\n",
       "      <td>33223.51</td>\n",
       "      <td>46104.87</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23374</th>\n",
       "      <td>9.48</td>\n",
       "      <td>93696.57</td>\n",
       "      <td>429.54</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.16</td>\n",
       "      <td>29636.65</td>\n",
       "      <td>-463.62</td>\n",
       "      <td>41413.73</td>\n",
       "      <td>...</td>\n",
       "      <td>50878.49</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>53.78</td>\n",
       "      <td>33223.42</td>\n",
       "      <td>-657.47</td>\n",
       "      <td>31967.08</td>\n",
       "      <td>33229.93</td>\n",
       "      <td>46109.90</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23375</th>\n",
       "      <td>5.80</td>\n",
       "      <td>80350.05</td>\n",
       "      <td>404.25</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>2.12</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.99</td>\n",
       "      <td>29642.65</td>\n",
       "      <td>-461.49</td>\n",
       "      <td>41412.40</td>\n",
       "      <td>...</td>\n",
       "      <td>50882.00</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>53.77</td>\n",
       "      <td>33231.23</td>\n",
       "      <td>-656.54</td>\n",
       "      <td>31965.70</td>\n",
       "      <td>33237.72</td>\n",
       "      <td>46114.55</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>43.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23376 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       proton_density  proton_temperature  proton_speed  bx_gsm  by_gsm  \\\n",
       "0                5.97            63349.11        374.65   -1.19    4.71   \n",
       "1                5.99            59161.81        370.63   -2.47    4.95   \n",
       "2                5.62            69938.55        377.34   -3.34    5.26   \n",
       "3                5.61            83242.99        403.80    2.07    7.53   \n",
       "4                5.70           170966.78        445.52   -1.93    8.20   \n",
       "...               ...                 ...           ...     ...     ...   \n",
       "23371            6.58            89799.83        438.73    1.41    2.39   \n",
       "23372            6.54            87216.37        427.42   -0.70    2.13   \n",
       "23373            8.22            76142.33        429.93    0.95    4.34   \n",
       "23374            9.48            93696.57        429.54    1.52    2.38   \n",
       "23375            5.80            80350.05        404.25   -2.07    2.12   \n",
       "\n",
       "       bz_gsm    bt     gn_X    gn_Y     gn_Z  ...     In_F  In_D  In_I  \\\n",
       "0       -1.53  5.35 29632.76 -111.92 41079.20  ... 50595.45 -8.18 53.53   \n",
       "1       -2.63  6.56 29628.98 -137.78 41075.50  ... 50587.19 -8.23 53.53   \n",
       "2        1.57  7.64 29632.27 -134.26 41078.96  ... 50592.18 -8.23 53.53   \n",
       "3       -6.16 11.85 29632.70 -126.09 41080.14  ... 50593.85 -8.21 53.54   \n",
       "4       -2.96 13.91 29585.53 -124.44 41082.73  ... 50568.52 -8.21 53.58   \n",
       "...       ...   ...      ...     ...      ...  ...      ...   ...   ...   \n",
       "23371    2.02  4.05 29628.33 -458.37 41412.76  ... 50872.34 -7.94 53.79   \n",
       "23372   -1.97  3.98 29628.82 -458.08 41412.40  ... 50872.96 -7.94 53.79   \n",
       "23373    1.72  5.40 29629.86 -458.85 41413.11  ... 50873.85 -7.94 53.78   \n",
       "23374    3.50  5.16 29636.65 -463.62 41413.73  ... 50878.49 -7.95 53.78   \n",
       "23375    4.83  5.99 29642.65 -461.49 41412.40  ... 50882.00 -7.94 53.77   \n",
       "\n",
       "          Jj_X    Jj_Y     Jj_Z     Jj_H     Jj_F  Jj_D  Jj_I  \n",
       "0     33215.99 -286.17 31620.31 33217.22 45860.96 -0.49 43.59  \n",
       "1     33209.00 -313.60 31614.71 33210.48 45852.22 -0.54 43.59  \n",
       "2     33210.12 -309.23 31621.87 33211.56 45857.95 -0.53 43.60  \n",
       "3     33211.10 -303.70 31622.53 33212.49 45859.06 -0.52 43.60  \n",
       "4     33161.93 -302.83 31623.31 33163.32 45824.00 -0.52 43.64  \n",
       "...        ...     ...      ...      ...      ...   ...   ...  \n",
       "23371 33215.36 -652.36 31963.44 33221.76 46101.49 -1.13 43.89  \n",
       "23372 33216.24 -652.49 31964.94 33222.64 46103.16 -1.13 43.89  \n",
       "23373 33217.10 -652.82 31966.49 33223.51 46104.87 -1.13 43.90  \n",
       "23374 33223.42 -657.47 31967.08 33229.93 46109.90 -1.13 43.89  \n",
       "23375 33231.23 -656.54 31965.70 33237.72 46114.55 -1.13 43.88  \n",
       "\n",
       "[23376 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3566fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=800,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    seed=2019\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "955e55ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpu_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m n_estimators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m]\n\u001b[1;32m     18\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m     20\u001b[0m param_distributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m---> 21\u001b[0m     treemethod\u001b[38;5;241m=\u001b[39m\u001b[43mgpu_hist\u001b[49m,\n\u001b[1;32m     22\u001b[0m     colsample_bytree\u001b[38;5;241m=\u001b[39mcolsample_bytree,\n\u001b[1;32m     23\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m     24\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     25\u001b[0m     min_child_weight\u001b[38;5;241m=\u001b[39mmin_child_weight,\n\u001b[1;32m     26\u001b[0m     subsample\u001b[38;5;241m=\u001b[39msubsample,\n\u001b[1;32m     27\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[1;32m     28\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m                                  param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[1;32m     34\u001b[0m                                  scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m                                 )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpu_hist' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost\n",
    "classifier = xgboost.XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4a3e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0da9025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[03:37:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.15\u001b[39m],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1500\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m2019\u001b[39m]\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     25\u001b[0m grid_mse \u001b[38;5;241m=\u001b[39m GridSearchCV(param_grid\u001b[38;5;241m=\u001b[39mparam_grid, estimator\u001b[38;5;241m=\u001b[39mgbm, \n\u001b[1;32m     26\u001b[0m                         scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mgrid_mse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_mse\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLowest RMSE found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mabs(grid_mse\u001b[38;5;241m.\u001b[39mbest_score_)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    505\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1250\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1230\u001b[0m model, feval, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, eval_metric, params)\n\u001b[1;32m   1231\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1232\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1233\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     label_transform\u001b[38;5;241m=\u001b[39mlabel_transform,\n\u001b[1;32m   1248\u001b[0m )\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, evals\u001b[38;5;241m=\u001b[39m(), obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m           maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, evals_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m           verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xgb_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124;03m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     bst \u001b[38;5;241m=\u001b[39m \u001b[43m_train_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1680\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#0.7215 ,1500\n",
    "# learning_rate=0.1,\n",
    "#     n_estimators=3000,\n",
    "#     max_depth=4,\n",
    "#     min_child_weight=1,\n",
    "#     gamma=0,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     nthread=-1,\n",
    "#     seed=2019\n",
    "gbm = XGBClassifier()\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01,0.05,0.1,0.15],\n",
    "    'n_estimators': [1500],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'nthread': [-1],\n",
    "    'seed':[2019]\n",
    "}\n",
    "grid_mse = GridSearchCV(param_grid=param_grid, estimator=gbm, \n",
    "                        scoring='neg_root_mean_squared_error', cv=2, verbose=1)\n",
    "\n",
    "grid_mse.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad2d8e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:41:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7704997100912119\n",
      "subsample = 0.6 colsample_bytree_list = 0.6\n",
      "[04:42:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.772440169350798\n",
      "subsample = 0.6 colsample_bytree_list = 0.7\n",
      "[04:42:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7801537432653138\n",
      "subsample = 0.6 colsample_bytree_list = 0.8\n",
      "[04:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7847999968608\n",
      "subsample = 0.6 colsample_bytree_list = 0.9\n",
      "[04:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7815231558397822\n",
      "subsample = 0.6 colsample_bytree_list = 1.0\n",
      "[04:44:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7785072685283109\n",
      "subsample = 0.7 colsample_bytree_list = 0.6\n",
      "[04:44:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7776827239701783\n",
      "subsample = 0.7 colsample_bytree_list = 0.7\n",
      "[04:45:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7729936915782988\n",
      "subsample = 0.7 colsample_bytree_list = 0.8\n",
      "[04:45:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7729936915782988\n",
      "subsample = 0.7 colsample_bytree_list = 0.9\n",
      "[04:46:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7765819693859519\n",
      "subsample = 0.7 colsample_bytree_list = 1.0\n",
      "[04:47:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7793309407056455\n",
      "subsample = 0.8 colsample_bytree_list = 0.6\n",
      "[04:47:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7768573042534529\n",
      "subsample = 0.8 colsample_bytree_list = 0.7\n",
      "[04:48:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7682760410672816\n",
      "subsample = 0.8 colsample_bytree_list = 0.8\n",
      "[04:48:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7718862501907473\n",
      "subsample = 0.8 colsample_bytree_list = 0.9\n",
      "[04:49:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7735468177253414\n",
      "subsample = 0.8 colsample_bytree_list = 1.0\n",
      "[04:50:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7809756789558916\n",
      "subsample = 0.9 colsample_bytree_list = 0.6\n",
      "[04:50:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7785072685283109\n",
      "subsample = 0.9 colsample_bytree_list = 0.7\n",
      "[04:51:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7765819693859519\n",
      "subsample = 0.9 colsample_bytree_list = 0.8\n",
      "[04:51:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7793309407056455\n",
      "subsample = 0.9 colsample_bytree_list = 0.9\n",
      "[04:52:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7809756789558916\n",
      "subsample = 0.9 colsample_bytree_list = 1.0\n",
      "[04:53:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7828901730745277\n",
      "subsample = 1.0 colsample_bytree_list = 0.6\n",
      "[04:53:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7801537432653138\n",
      "subsample = 1.0 colsample_bytree_list = 0.7\n",
      "[04:54:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7888769073951815\n",
      "subsample = 1.0 colsample_bytree_list = 0.8\n",
      "[04:54:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7842548074953415\n",
      "subsample = 1.0 colsample_bytree_list = 0.9\n",
      "[04:55:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "RMSE:  0.7823436528172943\n",
      "subsample = 1.0 colsample_bytree_list = 1.0\n"
     ]
    }
   ],
   "source": [
    "max_depth_lists = [3,4,5,6,7,8]\n",
    "min_child_weight_lists = [1,2,3,4,5,6]\n",
    "gamma_lists = [0,0.1,0.2,0.3,0.4,0.5]\n",
    "subsample_lists = [0.6,0.7,0.8,0.9,1.0]\n",
    "subsample_lists2 = [i/100.0 for i in range(71,90)]\n",
    "colsample_bytree_lists = [0.6,0.7,0.8,0.9,1.0]\n",
    "reg_alpha_lists = [1e-5,1e-4,1e-3,1e-2,0.1,1,10,100]\n",
    "n_estimator_lists = [i for i in range(100,6000,100)]\n",
    "learning_rate_lists = [0.05, 0.1,0.15,0.20,0.25,0.3]\n",
    "# for gamma_list in gamma_lists:\n",
    "    #0.7250 ,1500, 0.1\n",
    "    #1500, 0.1\n",
    "for subsample_list in subsample_lists:\n",
    "    for colsample_bytree_list in colsample_bytree_lists:\n",
    "# for reg_alpha_list in reg_alpha_lists:\n",
    "# for n_estimator_list in n_estimator_lists:\n",
    "#     for learning_rate_list in learning_rate_lists:\n",
    "\n",
    "# for max_depth_list in max_depth_lists:\n",
    "#     for min_child_weight_list in min_child_weight_lists:\n",
    "        \n",
    "        xgbc = XGBClassifier(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=700,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        nthread=-1,\n",
    "        seed=2019\n",
    "        )\n",
    "        xgbc.fit(X_train, y_train)\n",
    "        pred = xgbc.predict(X_test)\n",
    "\n",
    "        evaluate_regr(y_test, pred)\n",
    "#         print('gamma =', gamma_list)\n",
    "        print('subsample =', subsample_list, 'colsample_bytree_list =', colsample_bytree_list)\n",
    "    #     print('subsample =', subsample_list)\n",
    "    #     print('reg_alpha =', reg_alpha_list)\n",
    "#         print('n_estimator =', n_estimator_list, 'learning_rate =', learning_rate_list)\n",
    "#         print('max_depth=', max_depth_list, 'min_child_weight=', min_child_weight_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "02a7d2d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.8118561145468557\n",
      "min_samples_leaf = 1\n",
      "RMSE:  0.8163219479279898\n",
      "min_samples_leaf = 1\n",
      "RMSE:  0.8100100855860187\n",
      "min_samples_leaf = 1\n",
      "RMSE:  0.7988443320790239\n",
      "min_samples_leaf = 2\n",
      "RMSE:  0.8017837257372732\n",
      "min_samples_leaf = 2\n",
      "RMSE:  0.7940109438517664\n",
      "min_samples_leaf = 2\n",
      "RMSE:  0.795356498017879\n",
      "min_samples_leaf = 3\n",
      "RMSE:  0.8015169535702689\n",
      "min_samples_leaf = 3\n",
      "RMSE:  0.793472083291666\n",
      "min_samples_leaf = 3\n",
      "RMSE:  0.8071006522043631\n",
      "min_samples_leaf = 4\n",
      "RMSE:  0.7999144522266709\n",
      "min_samples_leaf = 4\n",
      "RMSE:  0.8049780958610502\n",
      "min_samples_leaf = 4\n",
      "RMSE:  0.8097460236114106\n",
      "min_samples_leaf = 5\n",
      "RMSE:  0.8028499279712591\n",
      "min_samples_leaf = 5\n",
      "RMSE:  0.811329106302714\n",
      "min_samples_leaf = 5\n",
      "RMSE:  0.8173691862648487\n",
      "min_samples_leaf = 6\n",
      "RMSE:  0.8244032499710771\n",
      "min_samples_leaf = 6\n",
      "RMSE:  0.8178923025976081\n",
      "min_samples_leaf = 6\n",
      "RMSE:  0.8205028815392424\n",
      "min_samples_leaf = 7\n",
      "RMSE:  0.8288014783437014\n",
      "min_samples_leaf = 7\n",
      "RMSE:  0.8210240012370873\n",
      "min_samples_leaf = 7\n",
      "RMSE:  0.8311205294503976\n",
      "min_samples_leaf = 8\n",
      "RMSE:  0.8249219057317067\n",
      "min_samples_leaf = 8\n",
      "RMSE:  0.8318921098698101\n",
      "min_samples_leaf = 8\n",
      "RMSE:  0.8321491443606863\n",
      "min_samples_leaf = 9\n",
      "RMSE:  0.8288014783437014\n",
      "min_samples_leaf = 9\n",
      "RMSE:  0.832662975310767\n",
      "min_samples_leaf = 9\n",
      "RMSE:  0.8377839520932258\n",
      "min_samples_leaf = 10\n",
      "RMSE:  0.8308631767489345\n",
      "min_samples_leaf = 10\n",
      "RMSE:  0.8336896871376616\n",
      "min_samples_leaf = 10\n",
      "RMSE:  0.8357393268312304\n",
      "min_samples_leaf = 11\n",
      "RMSE:  0.8385494012726258\n",
      "min_samples_leaf = 11\n",
      "RMSE:  0.841604236166631\n",
      "min_samples_leaf = 11\n",
      "RMSE:  0.837528646906102\n",
      "min_samples_leaf = 12\n",
      "RMSE:  0.837528646906102\n",
      "min_samples_leaf = 12\n",
      "RMSE:  0.8443947929114552\n",
      "min_samples_leaf = 12\n",
      "RMSE:  0.8474285564311121\n",
      "min_samples_leaf = 13\n",
      "RMSE:  0.8438881053213293\n",
      "min_samples_leaf = 13\n",
      "RMSE:  0.8464185101169549\n",
      "min_samples_leaf = 13\n",
      "RMSE:  0.8423662139018595\n",
      "min_samples_leaf = 14\n",
      "RMSE:  0.843381113322836\n",
      "min_samples_leaf = 14\n",
      "RMSE:  0.8466711346592513\n",
      "min_samples_leaf = 14\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "\n",
    "# max_depth_lists = [i for i in range(1,10)]\n",
    "min_samples_leaf_lists = [i for i in range(1,15)]\n",
    "# min_child_weight_lists = [1,2,3,4,5,6]\n",
    "# gamma_lists = [0,0.1,0.2,0.3,0.4,0.5]\n",
    "# subsample_lists = [0.6,0.7,0.8,0.9,1.0]\n",
    "# subsample_lists2 = [i/100.0 for i in range(71,90)]\n",
    "# colsample_bytree_lists = [0.6,0.7,0.8,0.9,1.0]\n",
    "# reg_alpha_lists = [1e-5,1e-4,1e-3,1e-2,0.1,1,10,100]\n",
    "# n_estimators_lists = [i for i in range(100,6000,100)]\n",
    "# learning_rate_lists = [0.05, 0.1,0.15,0.20,0.25,0.3]\n",
    "\n",
    "# for n_estimators in n_estimators_lists:\n",
    "for min_samples_leaf in min_samples_leaf_lists:\n",
    "    for i in range(3,6):\n",
    "        x = LinearDiscriminantAnalysis(\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "            )\n",
    "        x.fit(X_train, y_train)\n",
    "        pred = x.predict(X_test)\n",
    "        evaluate_regr(y_test, pred)\n",
    "#         print('n_estimators =', n_estimators)\n",
    "        print('min_samples_leaf =', min_samples_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b6a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_X, y_target, test_size=0.001, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b9c2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.7637626158259734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# 900 600 1600 2000\n",
    "y = LinearDiscriminantAnalysis()\n",
    "y.fit(X_train, y_train)\n",
    "pred = y.predict(X_test)\n",
    "evaluate_regr(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf387aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/quiz/preprocessed.csv')\n",
    "# submission = submission.drop(['Unnamed: 0'], axis=1, inplace=False)\n",
    "submission = submission[input_var]\n",
    "pred = y.predict(submission)\n",
    "submission['Dindex'] = pred\n",
    "submission[240:]['Dindex'].to_csv(\"data/submit.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
